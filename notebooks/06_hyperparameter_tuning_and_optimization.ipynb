{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3123551",
   "metadata": {},
   "source": "# Phase 6 â€” Hyperparameter Tuning and Optimization\n\n**Improved Approach:**\n1. Use **Optuna** for efficient Bayesian optimization (better than RandomizedSearchCV)\n2. Increase search iterations (20-30 trials)\n3. Use class weights for better handling of imbalanced data\n4. Cross-validation with 5 folds\n5. Optimize for F1-weighted score (better for imbalanced datasets)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport joblib\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Paths\nDATA_DIR = Path(\"../data/processed/ml_balance\")\nMODEL_DIR = Path(\"../trained_models\")\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load ORIGINAL data (use class weights instead of resampled data)\nprint(\"Loading original training data (for class weight approach)...\")\nX = pd.read_csv(DATA_DIR / \"train_original.csv\")\ny = pd.read_csv(DATA_DIR / \"train_original_labels.csv\")\n\nif y.shape[1] == 1:\n    y = y.iloc[:, 0]\n\n# Load class weights\nclass_weights = joblib.load(DATA_DIR / \"class_weights.pkl\")\nprint(f\"Loaded class weights for {len(class_weights)} classes\")\n\n# Define F1-weighted scorer\nf1_scorer = make_scorer(f1_score, average='weighted')\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Class distribution:\\n{y.value_counts()}\")\n\n# ============================================================\n# Random Forest Optimization with Optuna\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Random Forest Optimization with Optuna\")\nprint(\"=\" * 60)\n\ndef objective_rf(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 50),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n        'class_weight': 'balanced',\n        'random_state': 42,\n        'n_jobs': -1\n    }\n    \n    rf = RandomForestClassifier(**params)\n    \n    # 5-fold CV\n    scores = cross_val_score(rf, X, y, cv=5, scoring=f1_scorer, n_jobs=-1)\n    return scores.mean()\n\nstudy_rf = optuna.create_study(\n    direction='maximize',\n    sampler=TPESampler(seed=42),\n    study_name='RandomForest_Tuning'\n)\n\nstudy_rf.optimize(objective_rf, n_trials=25, show_progress_bar=True)\n\nprint(\"\\n--- Best RF Parameters ---\")\nprint(study_rf.best_params)\nprint(f\"Best F1 Score: {study_rf.best_value:.4f}\")\n\n# Train final model with best params\nbest_rf = RandomForestClassifier(**study_rf.best_params, random_state=42, n_jobs=-1)\nbest_rf.fit(X, y)\njoblib.dump(best_rf, MODEL_DIR / \"final_rf_optuna.pkl\")\nprint(f\"Saved to {MODEL_DIR / 'final_rf_optuna.pkl'}\")\n\n# ============================================================\n# XGBoost Optimization with Optuna\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"XGBoost Optimization with Optuna\")\nprint(\"=\" * 60)\n\n# Convert class weights to sample weights for XGBoost\nsample_weights = y.map(class_weights).values\n\ndef objective_xgb(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'random_state': 42,\n        'tree_method': 'hist',\n        'eval_metric': 'mlogloss'\n    }\n    \n    xgb = XGBClassifier(**params)\n    \n    # 5-fold CV with sample weights\n    scores = cross_val_score(\n        xgb, X, y, \n        cv=5, \n        scoring=f1_scorer,\n        fit_params={'sample_weight': sample_weights},\n        n_jobs=-1\n    )\n    return scores.mean()\n\nstudy_xgb = optuna.create_study(\n    direction='maximize',\n    sampler=TPESampler(seed=42),\n    study_name='XGBoost_Tuning'\n)\n\nstudy_xgb.optimize(objective_xgb, n_trials=30, show_progress_bar=True)\n\nprint(\"\\n--- Best XGBoost Parameters ---\")\nprint(study_xgb.best_params)\nprint(f\"Best F1 Score: {study_xgb.best_value:.4f}\")\n\n# Train final model with best params\nbest_xgb = XGBClassifier(**study_xgb.best_params, random_state=42, tree_method='hist', eval_metric='mlogloss')\nbest_xgb.fit(X, y, sample_weight=sample_weights)\njoblib.dump(best_xgb, MODEL_DIR / \"final_xgb_optuna.pkl\")\nprint(f\"Saved to {MODEL_DIR / 'final_xgb_optuna.pkl'}\")\n\n# ============================================================\n# LightGBM Optimization with Optuna\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"LightGBM Optimization with Optuna\")\nprint(\"=\" * 60)\n\ndef objective_lgbm(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n        'max_depth': trial.suggest_int('max_depth', 4, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'class_weight': 'balanced',\n        'random_state': 42,\n        'verbose': -1\n    }\n    \n    lgbm = LGBMClassifier(**params)\n    \n    # 5-fold CV\n    scores = cross_val_score(lgbm, X, y, cv=5, scoring=f1_scorer, n_jobs=-1)\n    return scores.mean()\n\nstudy_lgbm = optuna.create_study(\n    direction='maximize',\n    sampler=TPESampler(seed=42),\n    study_name='LightGBM_Tuning'\n)\n\nstudy_lgbm.optimize(objective_lgbm, n_trials=25, show_progress_bar=True)\n\nprint(\"\\n--- Best LightGBM Parameters ---\")\nprint(study_lgbm.best_params)\nprint(f\"Best F1 Score: {study_lgbm.best_value:.4f}\")\n\n# Train final model with best params\nbest_lgbm = LGBMClassifier(**study_lgbm.best_params, random_state=42, verbose=-1)\nbest_lgbm.fit(X, y)\njoblib.dump(best_lgbm, MODEL_DIR / \"final_lgbm_optuna.pkl\")\nprint(f\"Saved to {MODEL_DIR / 'final_lgbm_optuna.pkl'}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Optimization Complete!\")\nprint(\"=\" * 60)\nprint(\"\\nBest F1 Scores:\")\nprint(f\"  Random Forest: {study_rf.best_value:.4f}\")\nprint(f\"  XGBoost:       {study_xgb.best_value:.4f}\")\nprint(f\"  LightGBM:      {study_lgbm.best_value:.4f}\")",
    "import psutil\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "memory_utils_cell",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Memory Optimization Utilities\n",
    "# ===================================================================\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Reduce memory usage by optimizing data types\"\"\"\n",
    "    print(\"\\nOptimizing data types...\")\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    print(f\"  Initial memory: {start_mem:.2f} GB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    saved = start_mem - end_mem\n",
    "    print(f\"  Final memory: {end_mem:.2f} GB\")\n",
    "    print(f\"  Saved: {saved:.2f} GB ({100 * saved / start_mem:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"System RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "print(f\"Current process memory: {get_memory_usage():.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}