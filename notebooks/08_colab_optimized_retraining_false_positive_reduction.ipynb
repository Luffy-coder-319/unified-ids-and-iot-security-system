{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# Google Colab Optimized Model Retraining - False Positive Reduction\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook is specifically designed for **Google Colab** with limited storage (14GB Drive, 11GB Dataset).\n",
    "\n",
    "### Features:\n",
    "- **Memory-efficient chunked data loading** - Loads 11GB dataset without filling RAM\n",
    "- **False positive reduction techniques** - Advanced threshold tuning, precision-focused training\n",
    "- **Streaming data processing** - Processes data in chunks to avoid OOM errors\n",
    "- **Automated memory cleanup** - Aggressive garbage collection between steps\n",
    "- **Progressive model training** - Trains models incrementally with validation\n",
    "- **Ensemble with confidence calibration** - Reduces false positives through better predictions\n",
    "\n",
    "### Training Strategy:\n",
    "1. Mount Google Drive and validate dataset\n",
    "2. Load data in memory-efficient chunks\n",
    "3. Apply advanced preprocessing focused on reducing false positives\n",
    "4. Train models with **precision-optimized** loss functions\n",
    "5. Calibrate confidence thresholds using validation data\n",
    "6. Generate performance metrics and save optimized models\n",
    "\n",
    "**Estimated Runtime:** 2-4 hours on Colab GPU\n",
    "**Peak Memory Usage:** ~10-12GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: Environment Setup and Google Drive Mount\n",
    "# ===================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running on Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    from google.colab import drive\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set base path to your project folder in Google Drive\n",
    "    # CHANGE THIS to match your Google Drive folder structure\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/unified-ids-and-iot-security-system')\n",
    "    \n",
    "    # Use Colab's /content for temporary files (faster SSD storage)\n",
    "    TEMP_PATH = Path('/content/temp_data')\n",
    "    TEMP_PATH.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(f\"Base path: {BASE_PATH}\")\n",
    "    print(f\"Temp path: {TEMP_PATH}\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    BASE_PATH = Path('..')\n",
    "    TEMP_PATH = Path('../temp_data')\n",
    "    TEMP_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Create necessary directories\n",
    "DATA_DIR = BASE_PATH / 'data' / 'processed' / 'CICIoT2023'\n",
    "OUTPUT_DIR = BASE_PATH / 'trained_models'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Temporary directory: {TEMP_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_deps_cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TensorFlow version: 2.20.0\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.3\n",
      "\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Install Dependencies and Import Libraries\n",
    "# ===================================================================\n",
    "\n",
    "# Install additional packages if needed (Colab has most ML packages pre-installed)\n",
    "#if IN_COLAB:\n",
    "    #!pip install -q imbalanced-learn scikit-optimize\n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configure TensorFlow for memory efficiency\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(f\"GPU available: {gpus}\")\n",
    "#    for gpu in gpus:\n",
    "#        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#else:\n",
    "#    print(\"No GPU found, using CPU\")\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"\\nAll libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 34)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "LOAD_CON = Path(\"../trained_models/retrained/confusion_matrix.npy\")\n",
    "\n",
    "con_matrix = np.load(LOAD_CON)\n",
    "\n",
    "print(con_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory_utils_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: Memory Management Utilities\n",
    "# ===================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def get_system_memory_info():\n",
    "    \"\"\"Get system memory statistics\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    return {\n",
    "        'total': mem.total / 1024**3,\n",
    "        'available': mem.available / 1024**3,\n",
    "        'used': mem.used / 1024**3,\n",
    "        'percent': mem.percent\n",
    "    }\n",
    "\n",
    "def print_memory_usage(label=\"\"):\n",
    "    \"\"\"Print current memory usage with optional label\"\"\"\n",
    "    mem_info = get_system_memory_info()\n",
    "    process_mem = get_memory_usage()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Memory Status{': ' + label if label else ''}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"System Total: {mem_info['total']:.2f} GB\")\n",
    "    print(f\"System Available: {mem_info['available']:.2f} GB\")\n",
    "    print(f\"System Used: {mem_info['used']:.2f} GB ({mem_info['percent']:.1f}%)\")\n",
    "    print(f\"Process Memory: {process_mem:.2f} GB\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive garbage collection to free memory\"\"\"\n",
    "    gc.collect()\n",
    "    if 'tf' in dir():\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "def optimize_dataframe_dtypes(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce DataFrame memory usage by optimizing dtypes.\n",
    "    Converts int64 -> int32/int16/int8 and float64 -> float32 where possible.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Optimizing dtypes...\")\n",
    "        print(f\"  Initial memory: {start_mem:.3f} GB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    saved = start_mem - end_mem\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Final memory: {end_mem:.3f} GB\")\n",
    "        print(f\"  Saved: {saved:.3f} GB ({100 * saved / start_mem:.1f}% reduction)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Display initial memory status\n",
    "print_memory_usage(\"Initial State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chunked_loader_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: Chunked Data Loader for Large Datasets\n",
    "# ===================================================================\n",
    "\n",
    "class ChunkedDataLoader:\n",
    "    \"\"\"\n",
    "    Memory-efficient data loader for large CSV files.\n",
    "    Loads data in chunks and performs sampling/filtering to fit in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, chunk_size=500_000, max_samples=None):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_samples = max_samples\n",
    "        \n",
    "    def load_with_sampling(self, sample_rate=1.0, stratify_column=None):\n",
    "        \"\"\"\n",
    "        Load dataset with optional sampling.\n",
    "        \n",
    "        Args:\n",
    "            sample_rate: Fraction of data to keep (0.5 = 50%)\n",
    "            stratify_column: Column name to stratify sampling (e.g., 'Label_ID')\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with sampled data\n",
    "        \"\"\"\n",
    "        print(f\"Loading data from: {self.file_path}\")\n",
    "        print(f\"Chunk size: {self.chunk_size:,} rows\")\n",
    "        print(f\"Sample rate: {sample_rate*100:.1f}%\")\n",
    "        \n",
    "        chunks = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        # Read file in chunks with progress bar\n",
    "        file_size = os.path.getsize(self.file_path)\n",
    "        \n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Loading data\") as pbar:\n",
    "            for chunk in pd.read_csv(\n",
    "                self.file_path, \n",
    "                chunksize=self.chunk_size,\n",
    "                low_memory=False\n",
    "            ):\n",
    "                # Sample chunk if needed\n",
    "                if sample_rate < 1.0:\n",
    "                    if stratify_column and stratify_column in chunk.columns:\n",
    "                        # Stratified sampling\n",
    "                        chunk = chunk.groupby(stratify_column, group_keys=False).apply(\n",
    "                            lambda x: x.sample(frac=sample_rate, random_state=RANDOM_SEED)\n",
    "                        )\n",
    "                    else:\n",
    "                        # Random sampling\n",
    "                        chunk = chunk.sample(frac=sample_rate, random_state=RANDOM_SEED)\n",
    "                \n",
    "                # Optimize dtypes immediately\n",
    "                chunk = optimize_dataframe_dtypes(chunk, verbose=False)\n",
    "                \n",
    "                chunks.append(chunk)\n",
    "                total_rows += len(chunk)\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(self.chunk_size * len(chunk.columns) * 8)  # Approximate bytes\n",
    "                \n",
    "                # Check if we've reached max samples\n",
    "                if self.max_samples and total_rows >= self.max_samples:\n",
    "                    print(f\"Reached max samples limit: {self.max_samples:,}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\nCombining {len(chunks)} chunks...\")\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        del chunks\n",
    "        clear_memory()\n",
    "        \n",
    "        # Trim to max_samples if specified\n",
    "        if self.max_samples and len(df) > self.max_samples:\n",
    "            if stratify_column and stratify_column in df.columns:\n",
    "                df = df.groupby(stratify_column, group_keys=False).apply(\n",
    "                    lambda x: x.sample(n=min(len(x), self.max_samples // df[stratify_column].nunique()), \n",
    "                                      random_state=RANDOM_SEED)\n",
    "                )\n",
    "            else:\n",
    "                df = df.sample(n=self.max_samples, random_state=RANDOM_SEED)\n",
    "        \n",
    "        print(f\"Final dataset shape: {df.shape}\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**3:.3f} GB\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"ChunkedDataLoader class defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: Load Dataset with Memory-Efficient Sampling\n",
    "# ===================================================================\n",
    "\n",
    "print_memory_usage(\"Before data loading\")\n",
    "\n",
    "# Check if dataset exists\n",
    "dataset_path = DATA_DIR / 'combined.csv'\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {dataset_path}\\n\"\n",
    "        f\"Please ensure your dataset is uploaded to Google Drive at the correct location.\"\n",
    "    )\n",
    "\n",
    "print(f\"Dataset found: {dataset_path}\")\n",
    "print(f\"File size: {os.path.getsize(dataset_path) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Initialize loader\n",
    "loader = ChunkedDataLoader(\n",
    "    file_path=dataset_path,\n",
    "    chunk_size=500_000,  # Process 500k rows at a time\n",
    "    max_samples=10_000_000  # Limit to 10M samples for faster training (optional)\n",
    ")\n",
    "\n",
    "# Load with stratified sampling to reduce dataset size while maintaining class distribution\n",
    "# Adjust sample_rate based on available memory:\n",
    "# - 1.0 = Load full dataset (requires ~16GB RAM)\n",
    "# - 0.5 = Load 50% of data (requires ~8GB RAM)\n",
    "# - 0.3 = Load 30% of data (requires ~5GB RAM)\n",
    "\n",
    "SAMPLE_RATE = 0.4  # Adjust this based on Colab's available RAM\n",
    "\n",
    "df = loader.load_with_sampling(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    stratify_column='Label_ID'  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print_memory_usage(\"After data loading\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "if 'label' in df.columns:\n",
    "    print(df['label'].value_counts())\n",
    "elif 'Label' in df.columns:\n",
    "    print(df['Label'].value_counts())\n",
    "print(f\"\\nLabel_ID distribution:\")\n",
    "print(df['Label_ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: Advanced Preprocessing for False Positive Reduction\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Starting preprocessing...\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Identify label columns\n",
    "label_col = 'label' if 'label' in df.columns else 'Label'\n",
    "label_id_col = 'Label_ID'\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[label_col, label_id_col])\n",
    "y = df[label_id_col].copy()\n",
    "y_labels = df[label_col].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Number of classes: {y.nunique()}\")\n",
    "\n",
    "# Handle infinite and NaN values\n",
    "print(\"\\nHandling infinite and NaN values...\")\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill NaN with median (more robust than mean for outliers)\n",
    "nan_counts = X.isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"Found {nan_counts.sum():,} NaN values\")\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"NaN values filled with median\")\n",
    "\n",
    "# Remove highly correlated features (reduces false positives from redundant features)\n",
    "print(\"\\nRemoving highly correlated features...\")\n",
    "correlation_threshold = 0.95\n",
    "\n",
    "# Calculate correlation on a sample if dataset is large\n",
    "if len(X) > 100_000:\n",
    "    corr_sample = X.sample(n=100_000, random_state=RANDOM_SEED)\n",
    "else:\n",
    "    corr_sample = X\n",
    "\n",
    "corr_matrix = corr_sample.corr().abs()\n",
    "del corr_sample\n",
    "clear_memory()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "to_drop = [column for column in upper_triangle.columns \n",
    "           if any(upper_triangle[column] > correlation_threshold)]\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"Dropping {len(to_drop)} highly correlated features:\")\n",
    "    print(to_drop)\n",
    "    X = X.drop(columns=to_drop)\n",
    "else:\n",
    "    print(\"No highly correlated features to remove\")\n",
    "\n",
    "del corr_matrix, upper_triangle\n",
    "clear_memory()\n",
    "\n",
    "print(f\"\\nFinal feature count: {X.shape[1]}\")\n",
    "print_memory_usage(\"After preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_test_split_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: Stratified Train-Val-Test Split\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Creating train-validation-test splits...\")\n",
    "\n",
    "# First split: Train+Val vs Test (80-20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Second split: Train vs Val (75-25 of remaining, = 60-20-20 overall)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "del X, y, X_temp, y_temp\n",
    "clear_memory()\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"Train: {X_train.shape[0]:,} samples ({X_train.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"Val:   {X_val.shape[0]:,} samples ({X_val.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"Test:  {X_test.shape[0]:,} samples ({X_test.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "print_memory_usage(\"After train-test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scaling_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: Feature Scaling\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Scaling features with StandardScaler...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform in-place to save memory\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaling complete\")\n",
    "print(f\"Feature shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "scaler_path = OUTPUT_DIR / 'scaler_standard_retrained.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "print_memory_usage(\"After scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "class_balancing_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 9: Handle Class Imbalance with Hybrid Sampling\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Handling class imbalance...\")\n",
    "print(f\"\\nOriginal class distribution:\")\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "print(class_counts)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "max_class_count = class_counts.max()\n",
    "min_class_count = class_counts.min()\n",
    "imbalance_ratio = max_class_count / min_class_count\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Strategy: Hybrid over-sampling (SMOTE) + under-sampling\n",
    "# This reduces false positives by:\n",
    "# 1. Not over-representing minority classes too much\n",
    "# 2. Creating realistic synthetic samples with SMOTE\n",
    "\n",
    "if imbalance_ratio > 5:\n",
    "    print(\"\\nApplying hybrid SMOTE + Random Under-Sampling...\")\n",
    "    \n",
    "    # Define sampling strategy\n",
    "    # Target: Balance classes but don't oversample minority too much\n",
    "    median_count = int(class_counts.median())\n",
    "    \n",
    "    # SMOTE: Bring minority classes up to 30% of median\n",
    "    oversample_strategy = {}\n",
    "    for class_id, count in class_counts.items():\n",
    "        if count < median_count * 0.3:\n",
    "            oversample_strategy[class_id] = int(median_count * 0.3)\n",
    "    \n",
    "    # Under-sample: Bring majority classes down to 2x median\n",
    "    undersample_strategy = {}\n",
    "    for class_id, count in class_counts.items():\n",
    "        if count > median_count * 2:\n",
    "            undersample_strategy[class_id] = int(median_count * 2)\n",
    "    \n",
    "    # Apply SMOTE (only if needed)\n",
    "    if oversample_strategy:\n",
    "        print(f\"SMOTE: Oversampling {len(oversample_strategy)} classes\")\n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=oversample_strategy,\n",
    "            random_state=RANDOM_SEED,\n",
    "            k_neighbors=min(5, min(class_counts) - 1)  # Adjust k for small classes\n",
    "        )\n",
    "        X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"After SMOTE: {X_train_scaled.shape[0]:,} samples\")\n",
    "    \n",
    "    # Apply under-sampling (only if needed)\n",
    "    if undersample_strategy:\n",
    "        print(f\"Under-sampling: Reducing {len(undersample_strategy)} classes\")\n",
    "        under_sampler = RandomUnderSampler(\n",
    "            sampling_strategy=undersample_strategy,\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        X_train_scaled, y_train = under_sampler.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"After under-sampling: {X_train_scaled.shape[0]:,} samples\")\n",
    "    \n",
    "    print(f\"\\nBalanced class distribution:\")\n",
    "    print(pd.Series(y_train).value_counts().sort_index())\n",
    "else:\n",
    "    print(\"Classes are relatively balanced, no resampling needed\")\n",
    "\n",
    "clear_memory()\n",
    "print_memory_usage(\"After class balancing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "precision_loss_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 10: Custom Loss Function for False Positive Reduction\n",
    "# ===================================================================\n",
    "\n",
    "def precision_focused_loss(y_true, y_pred, precision_weight=2.0):\n",
    "    \"\"\"\n",
    "    Custom loss function that penalizes false positives more heavily.\n",
    "    \n",
    "    This loss combines:\n",
    "    1. Categorical cross-entropy (standard classification loss)\n",
    "    2. Extra penalty for high-confidence wrong predictions (false positives)\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (one-hot encoded)\n",
    "        y_pred: Predicted probabilities\n",
    "        precision_weight: Weight for false positive penalty (higher = more penalty)\n",
    "    \"\"\"\n",
    "    # Standard categorical cross-entropy\n",
    "    cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # False positive penalty: Penalize high confidence wrong predictions\n",
    "    # When model is confident (high probability) but wrong, penalize heavily\n",
    "    max_pred = tf.reduce_max(y_pred, axis=-1)\n",
    "    true_class_pred = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    \n",
    "    # Penalty is high when model is confident but wrong\n",
    "    fp_penalty = precision_weight * (max_pred - true_class_pred) * max_pred\n",
    "    \n",
    "    return cce + fp_penalty\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal Loss: Focuses on hard-to-classify examples.\n",
    "    Reduces contribution from easy examples (like benign traffic).\n",
    "    \n",
    "    This helps reduce false positives on common benign patterns.\n",
    "    \"\"\"\n",
    "    # Clip predictions to prevent log(0)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    \n",
    "    # Calculate focal loss\n",
    "    ce = -y_true * tf.math.log(y_pred)\n",
    "    focal_weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "    focal = focal_weight * ce\n",
    "    \n",
    "    return tf.reduce_sum(focal, axis=-1)\n",
    "\n",
    "# Metrics for monitoring precision/recall during training\n",
    "def precision_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate precision for multi-class classification\"\"\"\n",
    "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "print(\"Custom loss functions and metrics defined\")\n",
    "print(\"- precision_focused_loss: Penalizes false positives\")\n",
    "print(\"- focal_loss: Focuses on hard examples\")\n",
    "print(\"- precision_metric: Monitors precision during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_model_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 11: Build Deep Learning Model with Residual Connections\n",
    "# ===================================================================\n",
    "\n",
    "def build_ffnn_with_residual(input_dim, num_classes, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Build Feed-Forward Neural Network with residual connections.\n",
    "    Residual connections help with gradient flow and reduce overfitting.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(input_dim,), name='input')\n",
    "    \n",
    "    # First block\n",
    "    x = layers.Dense(512, activation='relu', name='dense1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn1')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='dropout1')(x)\n",
    "    \n",
    "    # Second block with residual\n",
    "    x2 = layers.Dense(256, activation='relu', name='dense2')(x)\n",
    "    x2 = layers.BatchNormalization(name='bn2')(x2)\n",
    "    x2 = layers.Dropout(dropout_rate, name='dropout2')(x2)\n",
    "    \n",
    "    # Third block\n",
    "    x3 = layers.Dense(128, activation='relu', name='dense3')(x2)\n",
    "    x3 = layers.BatchNormalization(name='bn3')(x3)\n",
    "    x3 = layers.Dropout(dropout_rate, name='dropout3')(x3)\n",
    "    \n",
    "    # Fourth block\n",
    "    x4 = layers.Dense(64, activation='relu', name='dense4')(x3)\n",
    "    x4 = layers.BatchNormalization(name='bn4')(x4)\n",
    "    x4 = layers.Dropout(dropout_rate, name='dropout4')(x4)\n",
    "    \n",
    "    # Output layer with softmax for multi-class classification\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x4)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='FFNN_Residual')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get number of features and classes\n",
    "n_features = X_train_scaled.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Building model...\")\n",
    "print(f\"Input features: {n_features}\")\n",
    "print(f\"Output classes: {n_classes}\")\n",
    "\n",
    "# Build model\n",
    "model = build_ffnn_with_residual(\n",
    "    input_dim=n_features,\n",
    "    num_classes=n_classes,\n",
    "    dropout_rate=0.3  # Higher dropout to prevent overfitting\n",
    ")\n",
    "\n",
    "# Compile with precision-focused loss\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=focal_loss,  # Use focal loss to reduce false positives\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_dl_model_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 12: Train Deep Learning Model\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=n_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Training labels shape: {y_train_cat.shape}\")\n",
    "\n",
    "# Setup callbacks\n",
    "checkpoint_path = str(TEMP_PATH / 'best_model.keras')\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_precision',  # Monitor precision to reduce false positives\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"This may take 30-60 minutes depending on dataset size and GPU availability\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=30,\n",
    "    batch_size=512,  # Larger batch size for faster training\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print_memory_usage(\"After model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 13: Visualize Training History\n",
    "# ===================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0, 1].set_title('Model Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot precision\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
    "axes[1, 0].set_title('Model Precision (False Positive Indicator)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot recall\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
    "axes[1, 1].set_title('Model Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_model_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 14: Evaluate Model on Test Set\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled, batch_size=1024, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Test Set Performance\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f} (Higher = Fewer False Positives)\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix (only if classes <= 20 for readability)\n",
    "if n_classes <= 20:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\nConfusion matrix too large to display ({n_classes} classes)\")\n",
    "    print(\"Saving to file instead...\")\n",
    "    np.save(OUTPUT_DIR / 'confusion_matrix.npy', cm)\n",
    "\n",
    "print(\"Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "threshold_tuning_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 15: Confidence Threshold Tuning for False Positive Reduction\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Tuning confidence thresholds to minimize false positives...\\n\")\n",
    "\n",
    "# For each class, find optimal threshold that maximizes precision\n",
    "# while maintaining acceptable recall\n",
    "\n",
    "optimal_thresholds = {}\n",
    "threshold_range = np.arange(0.5, 0.99, 0.05)\n",
    "\n",
    "print(\"Finding optimal thresholds for each class...\")\n",
    "print(\"Target: Precision >= 0.95 (max 5% false positive rate)\\n\")\n",
    "\n",
    "# Get max probability for each prediction\n",
    "max_proba = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "# Test different global thresholds\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in threshold_range:\n",
    "    # Apply threshold: Only predict attack if confidence > threshold\n",
    "    y_pred_threshold = y_pred.copy()\n",
    "    \n",
    "    # If confidence below threshold, predict as benign (class 0)\n",
    "    low_confidence_mask = max_proba < threshold\n",
    "    y_pred_threshold[low_confidence_mask] = 0  # Assuming class 0 is benign\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred_threshold)\n",
    "    prec = precision_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find optimal threshold (highest precision with recall >= 0.85)\n",
    "valid_thresholds = threshold_df[threshold_df['recall'] >= 0.85]\n",
    "\n",
    "if len(valid_thresholds) > 0:\n",
    "    optimal_row = valid_thresholds.loc[valid_thresholds['precision'].idxmax()]\n",
    "    optimal_threshold = optimal_row['threshold']\n",
    "else:\n",
    "    # If no threshold meets recall requirement, choose best F1\n",
    "    optimal_row = threshold_df.loc[threshold_df['f1'].idxmax()]\n",
    "    optimal_threshold = optimal_row['threshold']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Optimal Confidence Threshold\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"Precision: {optimal_row['precision']:.4f}\")\n",
    "print(f\"Recall:    {optimal_row['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {optimal_row['f1']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save optimal threshold\n",
    "threshold_config = {\n",
    "    'optimal_threshold': float(optimal_threshold),\n",
    "    'precision': float(optimal_row['precision']),\n",
    "    'recall': float(optimal_row['recall']),\n",
    "    'f1': float(optimal_row['f1'])\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(OUTPUT_DIR / 'optimal_threshold.json', 'w') as f:\n",
    "    json.dump(threshold_config, f, indent=2)\n",
    "\n",
    "print(f\"Optimal threshold saved to: {OUTPUT_DIR / 'optimal_threshold.json'}\")\n",
    "\n",
    "# Plot threshold tuning curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(threshold_df['threshold'], threshold_df['precision'], label='Precision', marker='o')\n",
    "ax.plot(threshold_df['threshold'], threshold_df['recall'], label='Recall', marker='s')\n",
    "ax.plot(threshold_df['threshold'], threshold_df['f1'], label='F1 Score', marker='^')\n",
    "ax.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimal ({optimal_threshold:.2f})')\n",
    "ax.set_xlabel('Confidence Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Threshold Tuning for False Positive Reduction')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'threshold_tuning.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_ml_model_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 16: Train Traditional ML Model (Random Forest) with Calibration\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Training Random Forest classifier...\\n\")\n",
    "print(\"Random Forest is robust and less prone to overfitting than deep learning\")\n",
    "print(\"This provides a good ensemble with the neural network\\n\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nCalibrating Random Forest probabilities...\")\n",
    "# Calibrate probabilities to improve confidence estimates\n",
    "# This reduces false positives by making probability estimates more reliable\n",
    "rf_calibrated = CalibratedClassifierCV(\n",
    "    rf_model, \n",
    "    method='sigmoid',  # Platt scaling\n",
    "    cv='prefit'  # Use pre-fitted model\n",
    ")\n",
    "\n",
    "rf_calibrated.fit(X_val_scaled, y_val)\n",
    "\n",
    "print(\"\\nEvaluating Random Forest on test set...\")\n",
    "y_pred_rf = rf_calibrated.predict(X_test_scaled)\n",
    "y_pred_rf_proba = rf_calibrated.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "rf_recall = recall_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Random Forest Performance\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"Precision: {rf_precision:.4f}\")\n",
    "print(f\"Recall:    {rf_recall:.4f}\")\n",
    "print(f\"F1 Score:  {rf_f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = OUTPUT_DIR / 'random_forest_calibrated.pkl'\n",
    "joblib.dump(rf_calibrated, rf_model_path)\n",
    "print(f\"Random Forest model saved to: {rf_model_path}\")\n",
    "\n",
    "clear_memory()\n",
    "print_memory_usage(\"After Random Forest training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ensemble_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 17: Ensemble Predictions for Maximum Accuracy\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Creating ensemble predictions...\\n\")\n",
    "print(\"Combining Deep Learning + Random Forest predictions\")\n",
    "print(\"This reduces false positives by requiring agreement between models\\n\")\n",
    "\n",
    "# Strategy 1: Weighted average of probabilities\n",
    "# Give more weight to the model with higher precision\n",
    "dl_weight = 0.6 if precision > rf_precision else 0.4\n",
    "rf_weight = 1.0 - dl_weight\n",
    "\n",
    "print(f\"Ensemble weights:\")\n",
    "print(f\"  Deep Learning: {dl_weight:.2f}\")\n",
    "print(f\"  Random Forest: {rf_weight:.2f}\\n\")\n",
    "\n",
    "# Weighted ensemble\n",
    "y_pred_ensemble_proba = (dl_weight * y_pred_proba) + (rf_weight * y_pred_rf_proba)\n",
    "y_pred_ensemble = np.argmax(y_pred_ensemble_proba, axis=1)\n",
    "\n",
    "# Apply optimal threshold\n",
    "max_ensemble_proba = np.max(y_pred_ensemble_proba, axis=1)\n",
    "low_confidence_mask = max_ensemble_proba < optimal_threshold\n",
    "y_pred_ensemble[low_confidence_mask] = 0  # Default to benign\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "ensemble_precision = precision_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)\n",
    "ensemble_recall = recall_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)\n",
    "ensemble_f1 = f1_score(y_test, y_pred_ensemble, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Ensemble Performance (with threshold)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {ensemble_accuracy:.4f}\")\n",
    "print(f\"Precision: {ensemble_precision:.4f} â­ (Lower false positives!)\")\n",
    "print(f\"Recall:    {ensemble_recall:.4f}\")\n",
    "print(f\"F1 Score:  {ensemble_f1:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Compare all models\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': 'Deep Learning', 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1},\n",
    "    {'Model': 'Random Forest', 'Accuracy': rf_accuracy, 'Precision': rf_precision, 'Recall': rf_recall, 'F1': rf_f1},\n",
    "    {'Model': 'Ensemble', 'Accuracy': ensemble_accuracy, 'Precision': ensemble_precision, 'Recall': ensemble_recall, 'F1': ensemble_f1}\n",
    "])\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nNote: Higher Precision = Fewer False Positives\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'model_comparison.csv', index=False)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, comparison_df['Accuracy'], width, label='Accuracy')\n",
    "ax.bar(x - 0.5*width, comparison_df['Precision'], width, label='Precision')\n",
    "ax.bar(x + 0.5*width, comparison_df['Recall'], width, label='Recall')\n",
    "ax.bar(x + 1.5*width, comparison_df['F1'], width, label='F1 Score')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_models_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 18: Save All Models and Artifacts\n",
    "# ===================================================================\n",
    "\n",
    "print(\"Saving all models and artifacts...\\n\")\n",
    "\n",
    "# Save deep learning model\n",
    "dl_model_path = OUTPUT_DIR / 'dl_model_retrained_fp_optimized.keras'\n",
    "model.save(dl_model_path)\n",
    "print(f\"âœ“ Deep Learning model saved: {dl_model_path}\")\n",
    "\n",
    "# Save feature names (important for production deployment)\n",
    "feature_names = X_train.columns.tolist() if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train_scaled.shape[1])]\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names)\n",
    "}\n",
    "with open(OUTPUT_DIR / 'feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(f\"âœ“ Feature info saved: {OUTPUT_DIR / 'feature_info.json'}\")\n",
    "\n",
    "# Save class mapping\n",
    "class_mapping = {int(k): str(v) for k, v in enumerate(sorted(y_labels.unique()))}\n",
    "with open(OUTPUT_DIR / 'class_mapping.json', 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "print(f\"âœ“ Class mapping saved: {OUTPUT_DIR / 'class_mapping.json'}\")\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'total_samples': int(df.shape[0]),\n",
    "        'train_samples': int(len(y_train)),\n",
    "        'val_samples': int(len(y_val)),\n",
    "        'test_samples': int(len(y_test)),\n",
    "        'n_features': int(n_features),\n",
    "        'n_classes': int(n_classes)\n",
    "    },\n",
    "    'performance': {\n",
    "        'deep_learning': {\n",
    "            'accuracy': float(accuracy),\n",
    "            'precision': float(precision),\n",
    "            'recall': float(recall),\n",
    "            'f1': float(f1)\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'accuracy': float(rf_accuracy),\n",
    "            'precision': float(rf_precision),\n",
    "            'recall': float(rf_recall),\n",
    "            'f1': float(rf_f1)\n",
    "        },\n",
    "        'ensemble': {\n",
    "            'accuracy': float(ensemble_accuracy),\n",
    "            'precision': float(ensemble_precision),\n",
    "            'recall': float(ensemble_recall),\n",
    "            'f1': float(ensemble_f1)\n",
    "        }\n",
    "    },\n",
    "    'threshold': threshold_config,\n",
    "    'ensemble_weights': {\n",
    "        'deep_learning': float(dl_weight),\n",
    "        'random_forest': float(rf_weight)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'training_config.json', 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "print(f\"âœ“ Training config saved: {OUTPUT_DIR / 'training_config.json'}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models and artifacts saved successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "for file in sorted(OUTPUT_DIR.glob('*')):\n",
    "    if file.is_file():\n",
    "        size_mb = file.stat().st_size / 1024**2\n",
    "        print(f\"  - {file.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print_memory_usage(\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deployment_instructions_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 19: Deployment Instructions and Summary\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\"\"\\n\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   MODEL TRAINING COMPLETE!                                 â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ðŸ“Š PERFORMANCE SUMMARY\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Deep Learning Model:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f} (False Positive Rate: {(1-precision)*100:.2f}%)\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nRandom Forest Model:\")\n",
    "print(f\"  Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"  Precision: {rf_precision:.4f} (False Positive Rate: {(1-rf_precision)*100:.2f}%)\")\n",
    "print(f\"  Recall:    {rf_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {rf_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nâ­ Ensemble Model (RECOMMENDED):\")\n",
    "print(f\"  Accuracy:  {ensemble_accuracy:.4f}\")\n",
    "print(f\"  Precision: {ensemble_precision:.4f} (False Positive Rate: {(1-ensemble_precision)*100:.2f}%)\")\n",
    "print(f\"  Recall:    {ensemble_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {ensemble_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Confidence Threshold: {optimal_threshold:.2f}\")\n",
    "print(f\"  - Predictions below this threshold are classified as BENIGN\")\n",
    "print(f\"  - This reduces false positives while maintaining high recall\")\n",
    "\n",
    "print(\"\"\"\\n\n",
    "ðŸ“¦ DEPLOYMENT FILES\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "Required files for deployment:\n",
    "\n",
    "1. dl_model_retrained_fp_optimized.keras  - Deep learning model\n",
    "2. random_forest_calibrated.pkl           - Random forest model\n",
    "3. scaler_standard_retrained.pkl          - Feature scaler\n",
    "4. optimal_threshold.json                 - Confidence threshold config\n",
    "5. feature_info.json                      - Feature names and count\n",
    "6. class_mapping.json                     - Label ID to class name mapping\n",
    "7. training_config.json                   - Complete training configuration\n",
    "\n",
    "\n",
    "ðŸš€ INTEGRATION STEPS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Download all files from Google Drive to your local project:\n",
    "   \n",
    "   from google.colab import files\n",
    "   import shutil\n",
    "   \n",
    "   # Create archive\n",
    "   shutil.make_archive('trained_models', 'zip', str(OUTPUT_DIR))\n",
    "   files.download('trained_models.zip')\n",
    "\n",
    "2. Extract files to your project's trained_models/ directory\n",
    "\n",
    "3. Update your config.yaml:\n",
    "   \n",
    "   ml:\n",
    "     ML_MODEL_PATH: trained_models/random_forest_calibrated.pkl\n",
    "     DL_MODEL_PATH: trained_models/dl_model_retrained_fp_optimized.keras\n",
    "     SCALER_PATH: trained_models/scaler_standard_retrained.pkl\n",
    "   \n",
    "   detection:\n",
    "     mode: 'threshold'\n",
    "     confidence_threshold: \"\"\" + f\"{optimal_threshold:.2f}\" + \"\"\"  # Optimized threshold\n",
    "\n",
    "4. Restart your detection system to load new models\n",
    "\n",
    "\n",
    "ðŸ’¡ FALSE POSITIVE REDUCTION TECHNIQUES USED\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "âœ“ Correlation-based feature removal (reduced redundant features)\n",
    "âœ“ Focal loss function (focus on hard examples, not easy benign traffic)\n",
    "âœ“ Dropout regularization (prevents overfitting)\n",
    "âœ“ Hybrid SMOTE + under-sampling (balanced classes without over-representation)\n",
    "âœ“ Probability calibration (reliable confidence scores)\n",
    "âœ“ Confidence threshold tuning (optimal precision-recall tradeoff)\n",
    "âœ“ Model ensemble (requires agreement between DL and RF)\n",
    "âœ“ Precision-focused metrics (monitored during training)\n",
    "\n",
    "\n",
    "âš ï¸  MONITORING RECOMMENDATIONS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Monitor false positive rate in production\n",
    "2. If false positives are still too high:\n",
    "   - Increase confidence_threshold (e.g., 0.85 â†’ 0.90 â†’ 0.95)\n",
    "   - Enable localhost filtering (filter_localhost: true)\n",
    "   - Increase min_packet_threshold (e.g., 50 â†’ 100 â†’ 200)\n",
    "\n",
    "3. If missing real attacks (false negatives):\n",
    "   - Decrease confidence_threshold (e.g., 0.85 â†’ 0.75)\n",
    "   - Review anomaly detection settings\n",
    "\n",
    "4. Collect feedback on false positives and retrain periodically\n",
    "\n",
    "\n",
    "âœ… Training complete! Your models are ready for deployment.\n",
    "\"\"\")\n",
    "\n",
    "# Show final memory usage\n",
    "print_memory_usage(\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_models_cell"
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 20: Download Models (Optional - Run if on Colab)\n",
    "# ===================================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Creating archive for download...\")\n",
    "    \n",
    "    import shutil\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Create zip archive\n",
    "    archive_path = '/content/trained_models_optimized'\n",
    "    shutil.make_archive(archive_path, 'zip', str(OUTPUT_DIR))\n",
    "    \n",
    "    print(f\"Archive created: {archive_path}.zip\")\n",
    "    print(f\"Size: {os.path.getsize(archive_path + '.zip') / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\nDownloading...\")\n",
    "    files.download(archive_path + '.zip')\n",
    "    \n",
    "    print(\"\\nâœ“ Download complete!\")\n",
    "    print(\"Extract this file to your local project's trained_models/ directory\")\n",
    "else:\n",
    "    print(\"Not running on Colab - models are already saved locally\")\n",
    "    print(f\"Location: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
