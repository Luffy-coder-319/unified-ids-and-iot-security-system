{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45248d4e",
   "metadata": {},
   "source": "# Phase 3 — Addressing Class Imbalance\n\nMultiple strategies:\n1. **Class weights** - Simpler, faster, preserves all real data\n2. **Moderate SMOTE** - Conservative oversampling for extremely rare classes\n3. **Focal Loss** - Advanced loss function for deep learning models\n\nWe'll create variants for different model types."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\nimport pandas as pd\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\nfrom sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport joblib\n\nDATA_DIR = Path(\"../data\")\nPROC_DIR = Path(\"../data/processed/ml_ready\")\nOUT_DIR = Path(\"../data/processed/ml_balance\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load standard-scaled training data\nX_train = pd.read_csv(PROC_DIR / \"X_train_standard.csv\").astype(\"float32\")\ny_train = pd.read_csv(PROC_DIR / \"y_train.csv\")\n\n# Ensure y_train is a Series\nif y_train.shape[1] == 1:\n    y_train = y_train.iloc[:, 0]\n\nprint(\"Original train label distribution:\\n\", y_train.value_counts())\nprint(\"\\nOriginal distribution:\", Counter(y_train))\n\n# ===============================================\n# Strategy 1: Compute Class Weights (for tree-based models)\n# ===============================================\nprint(\"\\n=== Computing Class Weights ===\")\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight('balanced', classes=classes, y=y_train)\nclass_weight_dict = dict(zip(classes, class_weights))\n\nprint(\"Class weights:\")\nfor cls, weight in class_weight_dict.items():\n    print(f\"  Class {cls}: {weight:.4f}\")\n\n# Save class weights\njoblib.dump(class_weight_dict, OUT_DIR / \"class_weights.pkl\")\nprint(\"Saved class weights to\", OUT_DIR / \"class_weights.pkl\")\n\n# ===============================================\n# Strategy 2: Moderate SMOTE (only for extremely rare classes)\n# ===============================================\nprint(\"\\n=== Applying Moderate SMOTE ===\")\n\n# Only boost classes with < 1000 samples to 5000\n# Cap BENIGN to reduce dominance\ntarget_distribution = {}\nfor cls, count in Counter(y_train).items():\n    if cls == 0:  # BENIGN\n        target_distribution[cls] = min(count, 200_000)\n    elif count < 1000:  # Very rare classes\n        target_distribution[cls] = 5_000\n    elif count < 10_000:  # Moderately rare\n        target_distribution[cls] = 10_000\n    else:  # Keep as is\n        target_distribution[cls] = count\n\nprint(\"Target distribution (moderate approach):\", target_distribution)\n\n# Apply moderate resampling\npipeline = Pipeline([\n    (\"under\", RandomUnderSampler(sampling_strategy={0: target_distribution[0]}, random_state=42)),\n    (\"smote\", SMOTE(random_state=42, sampling_strategy=target_distribution, k_neighbors=3))\n])\n\nX_res, y_res = pipeline.fit_resample(X_train, y_train)\n\nprint(\"\\nAfter moderate resampling:\", Counter(y_res))\n\n# Save balanced training set (for models that need resampled data)\npd.DataFrame(X_res, columns=X_train.columns).to_csv(OUT_DIR / \"train_balanced.csv\", index=False)\npd.Series(y_res).to_csv(OUT_DIR / \"train_balanced_labels.csv\", index=False)\n\n# ===============================================\n# Strategy 3: Save original training data (for class weight approach)\n# ===============================================\nprint(\"\\n=== Saving Original Data (for class weight training) ===\")\nX_train.to_csv(OUT_DIR / \"train_original.csv\", index=False)\ny_train.to_csv(OUT_DIR / \"train_original_labels.csv\", index=False)\n\n# Copy over test set\nX_test = pd.read_csv(PROC_DIR / \"X_test_standard.csv\").astype(\"float32\")\ny_test = pd.read_csv(PROC_DIR / \"y_test.csv\")\nif y_test.shape[1] == 1:\n    y_test = y_test.iloc[:, 0]\n\nX_test.to_csv(OUT_DIR / \"test.csv\", index=False)\ny_test.to_csv(OUT_DIR / \"test_labels.csv\", index=False)\n\nprint(\"\\nSaved all variants:\")\nprint(\"  - train_original.csv: Use with class_weights\")\nprint(\"  - train_balanced.csv: Use with moderate SMOTE\")\nprint(\"  - class_weights.pkl: For tree-based models\")\nprint(\"  - test.csv: Original test set (never resample!)\")\n\n# ===============================================\n# Strategy 4: Create Focal Loss for Deep Learning\n# ===============================================\nprint(\"\\n=== Creating Focal Loss Implementation ===\")\n\nfocal_loss_code = '''\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Focal Loss for multi-class classification\n    \n    Args:\n        gamma: Focusing parameter (default 2.0)\n        alpha: Balancing parameter (default 0.25)\n    \n    Returns:\n        Loss function\n    \"\"\"\n    def focal_loss_fixed(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n        \n        # Compute cross entropy\n        cross_entropy = -y_true * K.log(y_pred)\n        \n        # Compute focal loss\n        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n        \n        return K.mean(K.sum(loss, axis=-1))\n    \n    return focal_loss_fixed\n\n# Usage in model compilation:\n# model.compile(optimizer='adam', loss=focal_loss(gamma=2.0, alpha=0.25), metrics=['accuracy'])\n'''\n\n# Save focal loss code\nwith open(OUT_DIR / \"focal_loss.py\", \"w\") as f:\n    f.write(focal_loss_code)\n\nprint(\"Saved focal_loss.py for deep learning models\")\nprint(\"\\nRecommendations:\")\nprint(\"  • Tree models (RF/XGB): Use train_original.csv + class_weights\")\nprint(\"  • Deep Learning: Use train_original.csv + focal_loss.py\")\nprint(\"  • Baseline comparison: Use train_balanced.csv\")",
    "import psutil\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "memory_utils_cell",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Memory Optimization Utilities\n",
    "# ===================================================================\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Reduce memory usage by optimizing data types\"\"\"\n",
    "    print(\"\\nOptimizing data types...\")\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    print(f\"  Initial memory: {start_mem:.2f} GB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    saved = start_mem - end_mem\n",
    "    print(f\"  Final memory: {end_mem:.2f} GB\")\n",
    "    print(f\"  Saved: {saved:.2f} GB ({100 * saved / start_mem:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"System RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "print(f\"Current process memory: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0af95",
   "metadata": {},
   "outputs": [],
   "source": "# Visualization: Compare distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Original distribution\ny_train_counts = y_train.value_counts().sort_index()\naxes[0].bar(y_train_counts.index, y_train_counts.values, color='steelblue')\naxes[0].set_xlabel('Class')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Original Distribution (with class weights)')\naxes[0].set_yscale('log')\naxes[0].grid(alpha=0.3)\n\n# After moderate SMOTE\ny_res_counts = pd.Series(y_res).value_counts().sort_index()\naxes[1].bar(y_res_counts.index, y_res_counts.values, color='coral')\naxes[1].set_xlabel('Class')\naxes[1].set_ylabel('Count')\naxes[1].set_title('After Moderate SMOTE')\naxes[1].set_yscale('log')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSummary:\")\nprint(f\"Original samples: {len(y_train):,}\")\nprint(f\"After SMOTE: {len(y_res):,}\")\nprint(f\"Size increase: {(len(y_res) / len(y_train) - 1) * 100:.1f}%\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}