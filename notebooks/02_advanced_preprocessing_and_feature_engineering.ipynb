{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ca5757",
   "metadata": {},
   "source": "# Phase 2 — Advanced Preprocessing & Feature Engineering\n\nSplit before scaling, create Standard and MinMax scaled datasets, correlation analysis, feature importance, and save processed data for reproducibility.\n\n**Works with:** CIC-IoT-2023 or CICIDS2017 (auto-detects from Phase 1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport joblib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ===================================================================\n# Auto-detect dataset from Phase 1\n# ===================================================================\n# Check which dataset was processed in Phase 1\nif Path(\"../data/processed/CICIoT2023/combined.csv\").exists():\n    DATASET = \"CICIoT2023\"\n    DATA_DIR = Path(\"../data/processed/CICIoT2023\")\nelif Path(\"../data/processed/CICIDS2017/combined.csv\").exists():\n    DATASET = \"CICIDS2017\"\n    DATA_DIR = Path(\"../data/processed/CICIDS2017\")\nelse:\n    raise FileNotFoundError(\"No processed dataset found. Please run Notebook 01 first!\")\n\nOUT_DIR = Path(\"../data/processed/ml_ready\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Detected Dataset: {DATASET}\")\nprint(f\"Data Directory: {DATA_DIR}\")\n\n# ===================================================================\n# Load consolidated data created in Phase 1\n# ===================================================================\nprint(f\"\\nLoading consolidated data from Phase 1...\")\ndf = pd.read_csv(DATA_DIR / \"combined.csv\")\nprint(f\"Dataset shape: {df.shape}\")\n\nX = df.drop(columns=['label' if 'label' in df.columns else 'Label', 'Label_ID'])\ny = df['Label_ID']\n\nprint(f\"Features: {X.shape[1]}\")\nprint(f\"Samples: {X.shape[0]:,}\")\nprint(f\"Classes: {y.nunique()}\")\n\n# ===================================================================\n# Train-test split BEFORE scaling\n# ===================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Train-Test Split\")\nprint(\"=\" * 60)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\nprint(f\"Train shape: {X_train.shape}\")\nprint(f\"Test shape:  {X_test.shape}\")\n\n# ===================================================================\n# Correlation analysis on training set (original features)\n# ===================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Correlation Analysis\")\nprint(\"=\" * 60)\n\ncorr = X_train.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(corr, cmap='coolwarm', center=0, cbar_kws={'label': 'Correlation'})\nplt.title(f\"Feature Correlation - {DATASET} (Training Set)\")\nplt.tight_layout()\nplt.savefig(OUT_DIR / \"feature_correlation.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n# ===================================================================\n# Compute and remove highly correlated features\n# ===================================================================\nprint(\"\\nIdentifying highly correlated features...\")\nthreshold = 0.95\nhigh_corr = []\ncols = corr.columns\nto_drop = set()\n\nfor i in range(len(cols)):\n    for j in range(i):\n        if abs(corr.iloc[i,j]) > threshold:\n            high_corr.append((cols[i], cols[j], corr.iloc[i,j]))\n            # Drop the second feature in the pair\n            to_drop.add(cols[j])\n\nprint(f'High correlation pairs (>|0.95|): {len(high_corr)} pairs found')\n\nif to_drop:\n    print(f\"Dropping {len(to_drop)} highly correlated features:\")\n    for col in sorted(to_drop):\n        print(f\"  - {col}\")\n    \n    # Remove from both train and test\n    X_train = X_train.drop(columns=list(to_drop))\n    X_test = X_test.drop(columns=list(to_drop))\n    print(f\"\\nNew feature count: {X_train.shape[1]}\")\nelse:\n    print(\"No highly correlated features to remove\")\n\n# ===================================================================\n# Fit scalers on training data only (after feature removal)\n# ===================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Scaling Features\")\nprint(\"=\" * 60)\n\nstd_scaler = StandardScaler().fit(X_train)\nmm_scaler = MinMaxScaler().fit(X_train)\n\nX_train_std = std_scaler.transform(X_train)\nX_test_std = std_scaler.transform(X_test)\n\nX_train_mm = mm_scaler.transform(X_train)\nX_test_mm = mm_scaler.transform(X_test)\n\nprint(\"✓ StandardScaler fitted and applied\")\nprint(\"✓ MinMaxScaler fitted and applied\")\n\n# ===================================================================\n# Save scalers and processed arrays\n# ===================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Saving Processed Data\")\nprint(\"=\" * 60)\n\njoblib.dump(std_scaler, Path(\"../trained_models/scaler_standard.pkl\"))\njoblib.dump(mm_scaler, Path(\"../trained_models/scaler_minmax.pkl\"))\nprint(\"✓ Saved scalers\")\n\npd.DataFrame(X_train_std, columns=X_train.columns).to_csv(OUT_DIR / \"X_train_standard.csv\", index=False)\npd.DataFrame(X_test_std, columns=X_test.columns).to_csv(OUT_DIR / \"X_test_standard.csv\", index=False)\npd.DataFrame(X_train_mm, columns=X_train.columns).to_csv(OUT_DIR / \"X_train_minmax.csv\", index=False)\npd.DataFrame(X_test_mm, columns=X_test.columns).to_csv(OUT_DIR / \"X_test_minmax.csv\", index=False)\n\ny_train.to_csv(OUT_DIR / \"y_train.csv\", index=False)\ny_test.to_csv(OUT_DIR / \"y_test.csv\", index=False)\n\nprint(f\"✓ Saved processed data to {OUT_DIR}\")\n\n# Print file sizes\nfor file in OUT_DIR.glob(\"*.csv\"):\n    size_mb = file.stat().st_size / (1024**2)\n    print(f\"  {file.name}: {size_mb:.2f} MB\")\n\nprint(f\"\\n✓ Phase 2 complete!\")\nprint(f\"  Final feature count: {X_train.shape[1]}\")\nprint(f\"  Training samples: {X_train.shape[0]:,}\")\nprint(f\"  Test samples: {X_test.shape[0]:,}\")",
    "import psutil\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "memory_utils_cell",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Memory Optimization Utilities\n",
    "# ===================================================================\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Reduce memory usage by optimizing data types\"\"\"\n",
    "    print(\"\\nOptimizing data types...\")\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    print(f\"  Initial memory: {start_mem:.2f} GB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**3\n",
    "    saved = start_mem - end_mem\n",
    "    print(f\"  Final memory: {end_mem:.2f} GB\")\n",
    "    print(f\"  Saved: {saved:.2f} GB ({100 * saved / start_mem:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(f\"System RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "print(f\"Current process memory: {get_memory_usage():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c90119",
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance-based selection using Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\nprint(\"\\n=== Feature Importance Analysis ===\")\n# Train a quick RF to get feature importances\nrf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)\nrf_selector.fit(X_train_std, y_train)\n\n# Get feature importances\nimportances = rf_selector.feature_importances_\nfeature_names = X_train.columns\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 20 Most Important Features:\")\nprint(importance_df.head(20))\n\n# Optional: Select top N features or features above threshold\n# Uncomment to apply feature selection\n# top_n_features = 50\n# selected_features = importance_df.head(top_n_features)['feature'].tolist()\n# X_train = X_train[selected_features]\n# X_test = X_test[selected_features]\n# print(f\"\\nReduced to top {top_n_features} features\")\n\n# Save feature importance for reference\nimportance_df.to_csv(OUT_DIR / \"feature_importance.csv\", index=False)\nprint(\"\\nSaved feature importance to\", OUT_DIR / \"feature_importance.csv\")\n\n# Plot top 30 features\nplt.figure(figsize=(10, 12))\ntop_30 = importance_df.head(30)\nplt.barh(range(len(top_30)), top_30['importance'])\nplt.yticks(range(len(top_30)), top_30['feature'])\nplt.xlabel('Importance')\nplt.title('Top 30 Feature Importances')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}