{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518961dd",
   "metadata": {},
   "source": [
    "# Phase 7 — Model Comparison Dashboard\n",
    "\n",
    "Load all trained models and evaluate on a common test set. Produce a performance table and bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv(\"../data/test.csv\").values\n",
    "y_test = pd.read_csv(\"../data/test_labels.csv\", squeeze=True).values\n",
    "\n",
    "# Load classical models\n",
    "rf = joblib.load(\"../trained_models/final_rf.pkl\") if Path(\"../trained_models/final_rf.pkl\").exists() else joblib.load(\"../trained_models/best_baseline.pkl\")\n",
    "xgb = joblib.load(\"../trained_models/final_xgb.pkl\") if Path(\"../trained_models/final_xgb.pkl\").exists() else None\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate RF\n",
    "if rf is not None:\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    results.append({\n",
    "        'model': 'RandomForest',\n",
    "        'f1': f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        'accuracy': accuracy_score(y_test, y_pred_rf)\n",
    "    })\n",
    "\n",
    "# Evaluate XGB\n",
    "if xgb is not None:\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    results.append({\n",
    "        'model': 'XGBoost',\n",
    "        'f1': f1_score(y_test, y_pred_xgb, average='weighted'),\n",
    "        'accuracy': accuracy_score(y_test, y_pred_xgb)\n",
    "    })\n",
    "\n",
    "# Evaluate DL models\n",
    "num_classes = len(np.unique(y_test))\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "for name in ['final_ffnn.h5', 'final_cnn.h5', 'final_lstm.h5', 'final_autoencoder.h5']:\n",
    "    model_path = Path(\"../trained_models\") / name\n",
    "    if model_path.exists():\n",
    "        print(\"Loading\", name)\n",
    "        model = load_model(model_path)\n",
    "        if 'autoencoder' in name:\n",
    "            # For autoencoder, compute reconstruction error and set a threshold (simple approach)\n",
    "            recon = model.predict(X_test)\n",
    "            mse = np.mean(np.square(recon - X_test), axis=1)\n",
    "            threshold = np.percentile(mse, 95)\n",
    "            preds = (mse > threshold).astype(int)  # 1 = anomaly\n",
    "            # Map anomaly predictions to binary metrics — skip if multi-class expected\n",
    "            print(\"Autoencoder threshold (95th pct):\", threshold)\n",
    "        else:\n",
    "            if 'cnn' in name:\n",
    "                X_in = np.expand_dims(X_test, -1)\n",
    "            else:\n",
    "                X_in = X_test\n",
    "            y_prob = model.predict(X_in)\n",
    "            y_pred = np.argmax(y_prob, axis=1)\n",
    "            results.append({\n",
    "                'model': name.replace('.h5',''),\n",
    "                'f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "                'accuracy': accuracy_score(y_test, y_pred)\n",
    "            })\n",
    "\n",
    "res_df = pd.DataFrame(results).sort_values('f1', ascending=False).reset_index(drop=True)\n",
    "print(res_df)\n",
    "\n",
    "# Save comparison\n",
    "res_df.to_csv(\"../trained_models/model_comparison.csv\", index=False)\n",
    "print(\"Saved comparison to ../trained_models/model_comparison.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
