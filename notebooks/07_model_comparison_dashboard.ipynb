{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518961dd",
   "metadata": {},
   "source": [
    "# Phase 7 — Model Comparison Dashboard & Ensemble Methods\n",
    "\n",
    "**Enhanced Evaluation:**\n",
    "1. Load all trained models (classical ML + deep learning)\n",
    "2. Evaluate on common test set\n",
    "3. **Create ensemble models** (Voting, Stacking)\n",
    "4. **Comprehensive metrics**: Accuracy, F1-score, inference time, model size\n",
    "5. Per-class performance analysis\n",
    "6. Visual comparison dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861b72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Loads\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb8a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading Test Data\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test data: 100%|██████████| 2/2 [01:20<00:00, 40.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape: (9337316, 37)\n",
      "Test labels shape: (9337316,)\n",
      "Number of classes: 34\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load Test Data\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Test Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with tqdm(total=2, desc=\"Loading test data\") as pbar:\n",
    "    X_test = pd.read_csv(\"../data/processed/ml_balance/test.csv\", dtype=np.float32).values\n",
    "    pbar.update(1)\n",
    "    y_test = pd.read_csv(\"../data/processed/ml_balance/test_labels.csv\", dtype=np.int32).values\n",
    "    pbar.update(1)\n",
    "\n",
    "# Flatten y_test if needed\n",
    "if len(y_test.shape) > 1 and y_test.shape[1] == 1:\n",
    "    y_test = y_test.ravel()\n",
    "elif len(y_test.shape) > 1:\n",
    "    y_test = y_test.ravel()\n",
    "\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_test))}\")\n",
    "\n",
    "num_classes = len(np.unique(y_test))\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "MODEL_DIR = Path(\"../trained_models\")\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qob9xxnkth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Setting up Chunked Inference\n",
      "============================================================\n",
      "Chunk size set to: 100,000 samples\n",
      "Memory-efficient inference enabled\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Chunked Inference Helper Functions\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Setting up Chunked Inference\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "CHUNK_SIZE = 100000  # Adjust based on your memory constraints\n",
    "\n",
    "def load_data_in_chunks(file_path, chunk_size=CHUNK_SIZE, dtype=np.float32):\n",
    "    \"\"\"Load CSV data in chunks and return as a list of arrays\"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, dtype=dtype, chunksize=chunk_size):\n",
    "        chunks.append(chunk.values)\n",
    "    return chunks\n",
    "\n",
    "def predict_in_chunks(model, X_chunks, is_dl_model=False, reshape_for_cnn=False):\n",
    "    \"\"\"\n",
    "    Run inference in chunks to manage memory\n",
    "    \n",
    "    Args:\n",
    "        model: The model to use for prediction\n",
    "        X_chunks: List of data chunks or single array\n",
    "        is_dl_model: Whether this is a deep learning model\n",
    "        reshape_for_cnn: Whether to reshape input for CNN (add channel dimension)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Concatenated predictions\n",
    "        inference_time: Total inference time\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # If X_chunks is a single array, split it into chunks\n",
    "    if isinstance(X_chunks, np.ndarray):\n",
    "        n_samples = len(X_chunks)\n",
    "        n_chunks = int(np.ceil(n_samples / CHUNK_SIZE))\n",
    "        X_chunks = [X_chunks[i*CHUNK_SIZE:(i+1)*CHUNK_SIZE] for i in range(n_chunks)]\n",
    "    \n",
    "    for chunk in tqdm(X_chunks, desc=\"  Processing chunks\", leave=False):\n",
    "        # Prepare input for CNN if needed\n",
    "        if reshape_for_cnn:\n",
    "            chunk_input = np.expand_dims(chunk, -1)\n",
    "        else:\n",
    "            chunk_input = chunk\n",
    "        \n",
    "        # Run inference\n",
    "        if is_dl_model:\n",
    "            chunk_pred = model.predict(chunk_input, verbose=0)\n",
    "            # For classification, get argmax\n",
    "            if len(chunk_pred.shape) > 1 and chunk_pred.shape[1] > 1:\n",
    "                chunk_pred = np.argmax(chunk_pred, axis=1)\n",
    "        else:\n",
    "            chunk_pred = model.predict(chunk_input)\n",
    "        \n",
    "        predictions.append(chunk_pred)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del chunk_input\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    predictions = np.concatenate(predictions)\n",
    "    \n",
    "    return predictions, inference_time\n",
    "\n",
    "print(f\"Chunk size set to: {CHUNK_SIZE:,} samples\")\n",
    "print(\"Memory-efficient inference enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c1ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating Classical ML Models\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ML models:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading final/final_xgb_optuna.pkl...\n",
      "  Running chunked inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Load and Evaluate Classical ML Models (with Chunked Inference)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating Classical ML Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ml_models = {}\n",
    "ml_model_configs = [\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'files': ['final/final_xgb_optuna.pkl', 'xgboost_baseline.pkl'],\n",
    "        'key': 'xgb'\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightGBM',\n",
    "        'files': ['final/final_lgbm_optuna.pkl', 'lightgbm_baseline.pkl'],\n",
    "        'key': 'lgbm'\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in tqdm(ml_model_configs, desc=\"Loading ML models\"):\n",
    "    model_loaded = False\n",
    "    for model_name in config['files']:\n",
    "        model_path = MODEL_DIR / model_name\n",
    "        if model_path.exists():\n",
    "            print(f\"\\nLoading {model_name}...\")\n",
    "            try:\n",
    "                model = joblib.load(model_path)\n",
    "                \n",
    "                # Measure inference time with chunked prediction\n",
    "                print(\"  Running chunked inference...\")\n",
    "                y_pred, inference_time = predict_in_chunks(\n",
    "                    model, \n",
    "                    X_test, \n",
    "                    is_dl_model=False\n",
    "                )\n",
    "                \n",
    "                # Get model size\n",
    "                print(\"Getting model Size\")    \n",
    "                model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "                \n",
    "                # Calculate metrics\n",
    "                print(\"Calculating metrics\")\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_name.replace('.pkl', '').replace('final/', ''),\n",
    "                    'accuracy': acc,\n",
    "                    'f1_weighted': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'inference_time_sec': inference_time,\n",
    "                    'model_size_mb': model_size,\n",
    "                    'predictions_per_sec': len(X_test) / inference_time\n",
    "                })\n",
    "                \n",
    "                print(f\" Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n",
    "                \n",
    "                # Save for ensemble\n",
    "                ml_models[config['key']] = model\n",
    "                model_loaded = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error loading {model_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(f\"  No valid model found for {config['name']}\")\n",
    "\n",
    "print(f\"\\n Successfully loaded {len(ml_models)} ML models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed499b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load and Evaluate Deep Learning Models (with Chunked Inference)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluating Deep Learning Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dl_models = {}\n",
    "dl_model_configs = [\n",
    "    {\n",
    "        'name': 'FFNN Residual',\n",
    "        'files': ['final_ffnn_residual.keras'],\n",
    "        'key': 'ffnn',\n",
    "        'reshape': False\n",
    "    },\n",
    "    {\n",
    "        'name': 'CNN Stable',\n",
    "        'files': ['final_cnn_stable.keras'],\n",
    "        'key': 'cnn',\n",
    "        'reshape': True\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in tqdm(dl_model_configs, desc=\"Loading DL models\"):\n",
    "    model_loaded = False\n",
    "    for model_name in config['files']:\n",
    "        model_path = MODEL_DIR / 'dl_models' / model_name\n",
    "        if model_path.exists():\n",
    "            print(f\"\\nLoading {model_name}...\")\n",
    "            try:\n",
    "                model = load_model(model_path)\n",
    "                \n",
    "                # Measure inference time with chunked prediction\n",
    "                print(\"  Running chunked inference...\")\n",
    "                y_pred, inference_time = predict_in_chunks(\n",
    "                    model, \n",
    "                    X_test,\n",
    "                    is_dl_model=True,\n",
    "                    reshape_for_cnn=config['reshape']\n",
    "                )\n",
    "                \n",
    "                # Get model size\n",
    "                model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_name.replace('.keras', '').replace('.h5', ''),\n",
    "                    'accuracy': acc,\n",
    "                    'f1_weighted': f1,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'inference_time_sec': inference_time,\n",
    "                    'model_size_mb': model_size,\n",
    "                    'predictions_per_sec': len(X_test) / inference_time\n",
    "                })\n",
    "                \n",
    "                print(f\" Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n",
    "                \n",
    "                # Save for ensemble\n",
    "                dl_models[config['key']] = model\n",
    "                model_loaded = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(f\"No valid model found for {config['name']}\")\n",
    "\n",
    "print(f\"\\n Successfully loaded {len(ml_models)} ML models and {len(dl_models)} DL models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ie2rlpsgyv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization Dashboard\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Visual Comparison Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "with tqdm(total=7, desc=\"Creating dashboard\") as pbar:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Extract data\n",
    "    models = res_df['model']\n",
    "    f1_scores = res_df['f1_weighted']\n",
    "    colors = ['#2ecc71' if 'ensemble' in m else '#3498db' if any(x in m for x in ['ffnn', 'cnn']) else '#e74c3c' for m in models]\n",
    "    \n",
    "    # 1. F1 Score Comparison\n",
    "    ax = axes[0, 0]\n",
    "    ax.barh(models, f1_scores, color=colors)\n",
    "    ax.set_xlabel('F1 Score (Weighted)')\n",
    "    ax.set_title('F1 Score Comparison')\n",
    "    ax.set_xlim([0.9, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # 2. Accuracy Comparison\n",
    "    ax = axes[0, 1]\n",
    "    ax.barh(models, res_df['accuracy'], color=colors)\n",
    "    ax.set_xlabel('Accuracy')\n",
    "    ax.set_title('Accuracy Comparison')\n",
    "    ax.set_xlim([0.9, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # 3. Inference Time\n",
    "    ax = axes[0, 2]\n",
    "    ax.barh(models, res_df['inference_time_sec'], color=colors)\n",
    "    ax.set_xlabel('Inference Time (seconds)')\n",
    "    ax.set_title('Inference Speed (Lower is Better)')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # 4. Predictions per Second\n",
    "    ax = axes[1, 0]\n",
    "    ax.barh(models, res_df['predictions_per_sec'], color=colors)\n",
    "    ax.set_xlabel('Predictions/Second')\n",
    "    ax.set_title('Throughput (Higher is Better)')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # 5. Model Size\n",
    "    ax = axes[1, 1]\n",
    "    non_zero_sizes = res_df[res_df['model_size_mb'] > 0]\n",
    "    if len(non_zero_sizes) > 0:\n",
    "        ax.barh(non_zero_sizes['model'], non_zero_sizes['model_size_mb'], \n",
    "                color=[colors[i] for i in non_zero_sizes.index])\n",
    "        ax.set_xlabel('Model Size (MB)')\n",
    "        ax.set_title('Model Size Comparison')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No size data available', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title('Model Size Comparison')\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # 6. Precision vs Recall\n",
    "    ax = axes[1, 2]\n",
    "    ax.scatter(res_df['recall'], res_df['precision'], s=100, c=range(len(res_df)), cmap='viridis', alpha=0.7)\n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (res_df.iloc[i]['recall'], res_df.iloc[i]['precision']), \n",
    "                    fontsize=8, alpha=0.8, xytext=(5, 5), textcoords='offset points')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision vs Recall')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim([0.9, 1.0])\n",
    "    ax.set_ylim([0.9, 1.0])\n",
    "    pbar.update(1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'model_comparison_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSaved dashboard to {MODEL_DIR / 'model_comparison_dashboard.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aq02pf4sf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Per-Class Performance Analysis (Best Model)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Per-Class Performance Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get best model\n",
    "best_model_name = res_df.iloc[0]['model']\n",
    "print(f\"\\nAnalyzing best model: {best_model_name}\")\n",
    "\n",
    "# Load label encoder to get class names\n",
    "print(\"\\nLoading label encoder...\")\n",
    "try:\n",
    "    encoder = joblib.load(\"../trained_models/encoder.pkl\")\n",
    "    class_names = encoder.classes_\n",
    "    print(f\"Loaded {len(class_names)} class names\")\n",
    "except Exception as e:\n",
    "    print(f\" Could not load encoder: {e}\")\n",
    "    class_names = [f\"Class_{i}\" for i in range(num_classes)]\n",
    "\n",
    "# Get predictions from best model\n",
    "print(f\"\\nGetting predictions from {best_model_name}...\")\n",
    "with tqdm(total=1, desc=\"Running inference on best model\") as pbar:\n",
    "    if 'ensemble' in best_model_name:\n",
    "        best_model = joblib.load(MODEL_DIR / f\"{best_model_name}.pkl\")\n",
    "        y_pred_best = best_model.predict(X_test)\n",
    "    elif any(x in best_model_name for x in ['ffnn', 'cnn']):\n",
    "        # Find the model file\n",
    "        model_path = None\n",
    "        for ext in ['.keras', '.h5']:\n",
    "            potential_path = MODEL_DIR / 'dl_models' / f\"{best_model_name}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                model_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if model_path is None:\n",
    "            raise FileNotFoundError(f\"Could not find model file for {best_model_name}\")\n",
    "        \n",
    "        best_model = load_model(model_path)\n",
    "        if 'cnn' in best_model_name:\n",
    "            X_test_input = np.expand_dims(X_test, -1)\n",
    "        else:\n",
    "            X_test_input = X_test\n",
    "        y_prob = best_model.predict(X_test_input, verbose=0)\n",
    "        y_pred_best = np.argmax(y_prob, axis=1)\n",
    "    else:\n",
    "        best_model = joblib.load(MODEL_DIR / f\"{best_model_name}.pkl\")\n",
    "        y_pred_best = best_model.predict(X_test)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "report = classification_report(y_test, y_pred_best, target_names=class_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "# Per-class F1 scores\n",
    "print(\"\\nCalculating per-class metrics...\")\n",
    "with tqdm(total=2, desc=\"Analyzing per-class performance\") as pbar:\n",
    "    from sklearn.metrics import f1_score\n",
    "    per_class_f1 = f1_score(y_test, y_pred_best, average=None)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    class_performance = pd.DataFrame({\n",
    "        'class': class_names,\n",
    "        'f1_score': per_class_f1,\n",
    "        'support': [(y_test == i).sum() for i in range(num_classes)]\n",
    "    })\n",
    "    class_performance = class_performance.sort_values('f1_score')\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (sorted by performance):\")\n",
    "print(class_performance.to_string(index=False))\n",
    "\n",
    "# Plot per-class F1 scores\n",
    "print(\"\\nGenerating per-class visualization...\")\n",
    "with tqdm(total=1, desc=\"Creating per-class plot\") as pbar:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['#e74c3c' if f1 < 0.95 else '#f39c12' if f1 < 0.98 else '#2ecc71' for f1 in class_performance['f1_score']]\n",
    "    plt.barh(range(len(class_performance)), class_performance['f1_score'], color=colors)\n",
    "    plt.yticks(range(len(class_performance)), class_performance['class'])\n",
    "    plt.xlabel('F1 Score')\n",
    "    plt.title(f'Per-Class F1 Scores - {best_model_name}')\n",
    "    plt.axvline(x=0.95, color='red', linestyle='--', alpha=0.5, label='0.95 threshold')\n",
    "    plt.axvline(x=0.98, color='orange', linestyle='--', alpha=0.5, label='0.98 threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(MODEL_DIR / 'per_class_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    pbar.update(1)\n",
    "\n",
    "print(f\"\\n✓ Saved per-class analysis to {MODEL_DIR / 'per_class_performance.png'}\")\n",
    "\n",
    "# Identify weak classes\n",
    "weak_classes = class_performance[class_performance['f1_score'] < 0.95]\n",
    "if len(weak_classes) > 0:\n",
    "    print(\"\\nClasses with F1 < 0.95 (need improvement):\")\n",
    "    print(weak_classes.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n All classes have F1 >= 0.95!\")\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Per-Class Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best performing class: {class_performance.iloc[-1]['class']} (F1: {class_performance.iloc[-1]['f1_score']:.4f})\")\n",
    "print(f\"Worst performing class: {class_performance.iloc[0]['class']} (F1: {class_performance.iloc[0]['f1_score']:.4f})\")\n",
    "print(f\"Mean F1 score: {per_class_f1.mean():.4f}\")\n",
    "print(f\"Std F1 score: {per_class_f1.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prqepcqeod",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Results Summary Table\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nProcessing results...\")\n",
    "with tqdm(total=3, desc=\"Generating summary\") as pbar:\n",
    "    # Sort by F1 score\n",
    "    res_df = pd.DataFrame(results).sort_values('f1_weighted', ascending=False).reset_index(drop=True)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Round for display\n",
    "    display_df = res_df.copy()\n",
    "    display_df['accuracy'] = display_df['accuracy'].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['f1_weighted'] = display_df['f1_weighted'].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['precision'] = display_df['precision'].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['recall'] = display_df['recall'].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['inference_time_sec'] = display_df['inference_time_sec'].apply(lambda x: f\"{x:.2f}\")\n",
    "    display_df['model_size_mb'] = display_df['model_size_mb'].apply(lambda x: f\"{x:.2f}\")\n",
    "    display_df['predictions_per_sec'] = display_df['predictions_per_sec'].apply(lambda x: f\"{x:.0f}\")\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Save comparison\n",
    "    res_df.to_csv(MODEL_DIR / \"model_comparison_enhanced.csv\", index=False)\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSaved detailed comparison to {MODEL_DIR / 'model_comparison_enhanced.csv'}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total models evaluated: {len(res_df)}\")\n",
    "print(f\"Best F1 Score: {res_df.iloc[0]['f1_weighted']:.4f} ({res_df.iloc[0]['model']})\")\n",
    "print(f\"Best Accuracy: {res_df['accuracy'].max():.4f}\")\n",
    "print(f\"Fastest inference: {res_df['inference_time_sec'].min():.2f}s\")\n",
    "print(f\"Highest throughput: {res_df['predictions_per_sec'].max():.0f} predictions/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
