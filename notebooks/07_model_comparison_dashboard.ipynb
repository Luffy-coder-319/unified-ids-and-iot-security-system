{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518961dd",
   "metadata": {},
   "source": "# Phase 7 — Model Comparison Dashboard & Ensemble Methods\n\n**Enhanced Evaluation:**\n1. Load all trained models (classical ML + deep learning)\n2. Evaluate on common test set\n3. **Create ensemble models** (Voting, Stacking)\n4. **Comprehensive metrics**: Accuracy, F1-score, inference time, model size\n5. Per-class performance analysis\n6. Visual comparison dashboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b72cd",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport joblib\nimport time\nimport os\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.utils import to_categorical\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load test data\nX_test = pd.read_csv(\"../data/processed/ml_balance/test.csv\").values\ny_test = pd.read_csv(\"../data/processed/ml_balance/test_labels.csv\").values\n\n# Flatten y_test if needed\nif len(y_test.shape) > 1 and y_test.shape[1] == 1:\n    y_test = y_test.ravel()\nelif len(y_test.shape) > 1:\n    y_test = y_test.ravel()\n\nprint(f\"Test set shape: {X_test.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")\nprint(f\"Number of classes: {len(np.unique(y_test))}\")\n\nnum_classes = len(np.unique(y_test))\ny_test_cat = to_categorical(y_test, num_classes)\n\nMODEL_DIR = Path(\"../trained_models\")\nresults = []\n\n# ============================================================\n# Load and Evaluate Classical ML Models\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Evaluating Classical ML Models\")\nprint(\"=\" * 60)\n\nml_models = {}\n\n# Random Forest\nfor model_name in ['final_rf_optuna.pkl', 'final_rf.pkl', 'best_baseline.pkl']:\n    model_path = MODEL_DIR / model_name\n    if model_path.exists():\n        print(f\"\\nLoading {model_name}...\")\n        model = joblib.load(model_path)\n        \n        # Measure inference time\n        start = time.time()\n        y_pred = model.predict(X_test)\n        inference_time = time.time() - start\n        \n        # Get model size\n        model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n        \n        # Calculate metrics\n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted')\n        \n        results.append({\n            'model': model_name.replace('.pkl', ''),\n            'accuracy': acc,\n            'f1_weighted': f1,\n            'precision': precision,\n            'recall': recall,\n            'inference_time_sec': inference_time,\n            'model_size_mb': model_size,\n            'predictions_per_sec': len(X_test) / inference_time\n        })\n        \n        print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n        \n        # Save for ensemble\n        if 'rf' in model_name.lower():\n            ml_models['rf'] = model\n        \n        break  # Use first available\n\n# XGBoost\nfor model_name in ['final_xgb_optuna.pkl', 'final_xgb.pkl']:\n    model_path = MODEL_DIR / model_name\n    if model_path.exists():\n        print(f\"\\nLoading {model_name}...\")\n        model = joblib.load(model_path)\n        \n        start = time.time()\n        y_pred = model.predict(X_test)\n        inference_time = time.time() - start\n        \n        model_size = os.path.getsize(model_path) / (1024 * 1024)\n        \n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted')\n        \n        results.append({\n            'model': model_name.replace('.pkl', ''),\n            'accuracy': acc,\n            'f1_weighted': f1,\n            'precision': precision,\n            'recall': recall,\n            'inference_time_sec': inference_time,\n            'model_size_mb': model_size,\n            'predictions_per_sec': len(X_test) / inference_time\n        })\n        \n        print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n        \n        ml_models['xgb'] = model\n        break\n\n# LightGBM\nfor model_name in ['final_lgbm_optuna.pkl', 'final_lgbm.pkl']:\n    model_path = MODEL_DIR / model_name\n    if model_path.exists():\n        print(f\"\\nLoading {model_name}...\")\n        model = joblib.load(model_path)\n        \n        start = time.time()\n        y_pred = model.predict(X_test)\n        inference_time = time.time() - start\n        \n        model_size = os.path.getsize(model_path) / (1024 * 1024)\n        \n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted')\n        \n        results.append({\n            'model': model_name.replace('.pkl', ''),\n            'accuracy': acc,\n            'f1_weighted': f1,\n            'precision': precision,\n            'recall': recall,\n            'inference_time_sec': inference_time,\n            'model_size_mb': model_size,\n            'predictions_per_sec': len(X_test) / inference_time\n        })\n        \n        print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n        \n        ml_models['lgbm'] = model\n        break\n\n# ============================================================\n# Load and Evaluate Deep Learning Models\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Evaluating Deep Learning Models\")\nprint(\"=\" * 60)\n\ndl_models = {}\n\n# FFNN Residual\nfor model_name in ['final_ffnn_residual.keras', 'final_ffnn.keras', 'final_ffnn.h5']:\n    model_path = MODEL_DIR / 'dl_models' / model_name\n    if model_path.exists():\n        print(f\"\\nLoading {model_name}...\")\n        model = load_model(model_path)\n        \n        start = time.time()\n        y_prob = model.predict(X_test, verbose=0)\n        y_pred = np.argmax(y_prob, axis=1)\n        inference_time = time.time() - start\n        \n        model_size = os.path.getsize(model_path) / (1024 * 1024)\n        \n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted')\n        \n        results.append({\n            'model': model_name.replace('.keras', '').replace('.h5', ''),\n            'accuracy': acc,\n            'f1_weighted': f1,\n            'precision': precision,\n            'recall': recall,\n            'inference_time_sec': inference_time,\n            'model_size_mb': model_size,\n            'predictions_per_sec': len(X_test) / inference_time\n        })\n        \n        print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n        \n        dl_models['ffnn'] = model\n        break\n\n# CNN Stable\nfor model_name in ['final_cnn_stable.keras', 'final_cnn.keras', 'final_cnn.h5']:\n    model_path = MODEL_DIR / 'dl_models' / model_name\n    if model_path.exists():\n        print(f\"\\nLoading {model_name}...\")\n        model = load_model(model_path)\n        \n        X_test_cnn = np.expand_dims(X_test, -1)\n        \n        start = time.time()\n        y_prob = model.predict(X_test_cnn, verbose=0)\n        y_pred = np.argmax(y_prob, axis=1)\n        inference_time = time.time() - start\n        \n        model_size = os.path.getsize(model_path) / (1024 * 1024)\n        \n        acc = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n        recall = recall_score(y_test, y_pred, average='weighted')\n        \n        results.append({\n            'model': model_name.replace('.keras', '').replace('.h5', ''),\n            'accuracy': acc,\n            'f1_weighted': f1,\n            'precision': precision,\n            'recall': recall,\n            'inference_time_sec': inference_time,\n            'model_size_mb': model_size,\n            'predictions_per_sec': len(X_test) / inference_time\n        })\n        \n        print(f\"  Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {inference_time:.2f}s\")\n        \n        dl_models['cnn'] = model\n        break\n\nprint(f\"\\nLoaded {len(ml_models)} ML models and {len(dl_models)} DL models\")"
  },
  {
   "cell_type": "code",
   "id": "e2ux3hlus8g",
   "source": "# ============================================================\n# Create Ensemble Models\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Creating Ensemble Models\")\nprint(\"=\" * 60)\n\nif len(ml_models) >= 2:\n    # Voting Classifier (Soft voting - uses probabilities)\n    print(\"\\n--- Soft Voting Ensemble ---\")\n    estimators = [(name, model) for name, model in ml_models.items()]\n    \n    voting_clf = VotingClassifier(\n        estimators=estimators,\n        voting='soft',\n        n_jobs=-1\n    )\n    \n    # Fit on training data (required for voting)\n    print(\"Fitting voting ensemble...\")\n    X_train = pd.read_csv(\"../data/processed/ml_balance/train_original.csv\").values\n    y_train = pd.read_csv(\"../data/processed/ml_balance/train_original_labels.csv\").values\n    if len(y_train.shape) > 1:\n        y_train = y_train.ravel()\n    \n    voting_clf.fit(X_train, y_train)\n    \n    # Evaluate\n    start = time.time()\n    y_pred_voting = voting_clf.predict(X_test)\n    inference_time = time.time() - start\n    \n    acc = accuracy_score(y_test, y_pred_voting)\n    f1 = f1_score(y_test, y_pred_voting, average='weighted')\n    precision = precision_score(y_test, y_pred_voting, average='weighted', zero_division=0)\n    recall = recall_score(y_test, y_pred_voting, average='weighted')\n    \n    results.append({\n        'model': 'voting_ensemble',\n        'accuracy': acc,\n        'f1_weighted': f1,\n        'precision': precision,\n        'recall': recall,\n        'inference_time_sec': inference_time,\n        'model_size_mb': 0,  # Combined size\n        'predictions_per_sec': len(X_test) / inference_time\n    })\n    \n    print(f\"Voting Ensemble - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n    \n    # Save voting ensemble\n    joblib.dump(voting_clf, MODEL_DIR / \"voting_ensemble.pkl\")\n    print(f\"Saved to {MODEL_DIR / 'voting_ensemble.pkl'}\")\n    \n    # Stacking Classifier\n    print(\"\\n--- Stacking Ensemble ---\")\n    stacking_clf = StackingClassifier(\n        estimators=estimators,\n        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n        cv=5,\n        n_jobs=-1\n    )\n    \n    print(\"Fitting stacking ensemble (this may take a while)...\")\n    stacking_clf.fit(X_train, y_train)\n    \n    # Evaluate\n    start = time.time()\n    y_pred_stacking = stacking_clf.predict(X_test)\n    inference_time = time.time() - start\n    \n    acc = accuracy_score(y_test, y_pred_stacking)\n    f1 = f1_score(y_test, y_pred_stacking, average='weighted')\n    precision = precision_score(y_test, y_pred_stacking, average='weighted', zero_division=0)\n    recall = recall_score(y_test, y_pred_stacking, average='weighted')\n    \n    results.append({\n        'model': 'stacking_ensemble',\n        'accuracy': acc,\n        'f1_weighted': f1,\n        'precision': precision,\n        'recall': recall,\n        'inference_time_sec': inference_time,\n        'model_size_mb': 0,\n        'predictions_per_sec': len(X_test) / inference_time\n    })\n    \n    print(f\"Stacking Ensemble - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n    \n    # Save stacking ensemble\n    joblib.dump(stacking_clf, MODEL_DIR / \"stacking_ensemble.pkl\")\n    print(f\"Saved to {MODEL_DIR / 'stacking_ensemble.pkl'}\")\n    \nelse:\n    print(\"Not enough ML models for ensemble (need at least 2)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ie2rlpsgyv",
   "source": "# ============================================================\n# Visualization Dashboard\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Visual Comparison Dashboard\")\nprint(\"=\" * 60)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# 1. F1 Score Comparison\nax = axes[0, 0]\nmodels = res_df['model']\nf1_scores = res_df['f1_weighted']\ncolors = ['#2ecc71' if 'ensemble' in m else '#3498db' if any(x in m for x in ['ffnn', 'cnn']) else '#e74c3c' for m in models]\nax.barh(models, f1_scores, color=colors)\nax.set_xlabel('F1 Score (Weighted)')\nax.set_title('F1 Score Comparison')\nax.set_xlim([0.9, 1.0])\nax.grid(axis='x', alpha=0.3)\n\n# 2. Accuracy Comparison\nax = axes[0, 1]\nax.barh(models, res_df['accuracy'], color=colors)\nax.set_xlabel('Accuracy')\nax.set_title('Accuracy Comparison')\nax.set_xlim([0.9, 1.0])\nax.grid(axis='x', alpha=0.3)\n\n# 3. Inference Time\nax = axes[0, 2]\nax.barh(models, res_df['inference_time_sec'], color=colors)\nax.set_xlabel('Inference Time (seconds)')\nax.set_title('Inference Speed (Lower is Better)')\nax.grid(axis='x', alpha=0.3)\n\n# 4. Predictions per Second\nax = axes[1, 0]\nax.barh(models, res_df['predictions_per_sec'], color=colors)\nax.set_xlabel('Predictions/Second')\nax.set_title('Throughput (Higher is Better)')\nax.grid(axis='x', alpha=0.3)\n\n# 5. Model Size\nax = axes[1, 1]\nnon_zero_sizes = res_df[res_df['model_size_mb'] > 0]\nax.barh(non_zero_sizes['model'], non_zero_sizes['model_size_mb'], \n        color=[colors[i] for i in non_zero_sizes.index])\nax.set_xlabel('Model Size (MB)')\nax.set_title('Model Size Comparison')\nax.grid(axis='x', alpha=0.3)\n\n# 6. Precision vs Recall\nax = axes[1, 2]\nax.scatter(res_df['recall'], res_df['precision'], s=100, c=range(len(res_df)), cmap='viridis', alpha=0.7)\nfor i, model in enumerate(models):\n    ax.annotate(model, (res_df.iloc[i]['recall'], res_df.iloc[i]['precision']), \n                fontsize=8, alpha=0.8, xytext=(5, 5), textcoords='offset points')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_title('Precision vs Recall')\nax.grid(alpha=0.3)\nax.set_xlim([0.9, 1.0])\nax.set_ylim([0.9, 1.0])\n\nplt.tight_layout()\nplt.savefig(MODEL_DIR / 'model_comparison_dashboard.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nSaved dashboard to {MODEL_DIR / 'model_comparison_dashboard.png'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aq02pf4sf16",
   "source": "# ============================================================\n# Per-Class Performance Analysis (Best Model)\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Per-Class Performance Analysis\")\nprint(\"=\" * 60)\n\n# Get best model\nbest_model_name = res_df.iloc[0]['model']\nprint(f\"\\nAnalyzing best model: {best_model_name}\")\n\n# Load label encoder to get class names\ntry:\n    encoder = joblib.load(\"../trained_models/encoder.pkl\")\n    class_names = encoder.classes_\nexcept:\n    class_names = [f\"Class_{i}\" for i in range(num_classes)]\n\n# Get predictions from best model\nif 'ensemble' in best_model_name:\n    best_model = joblib.load(MODEL_DIR / f\"{best_model_name}.pkl\")\n    y_pred_best = best_model.predict(X_test)\nelif any(x in best_model_name for x in ['ffnn', 'cnn']):\n    model_path = MODEL_DIR / 'dl_models' / f\"{best_model_name}.keras\"\n    if not model_path.exists():\n        model_path = MODEL_DIR / 'dl_models' / f\"{best_model_name}.h5\"\n    best_model = load_model(model_path)\n    if 'cnn' in best_model_name:\n        X_test_input = np.expand_dims(X_test, -1)\n    else:\n        X_test_input = X_test\n    y_prob = best_model.predict(X_test_input, verbose=0)\n    y_pred_best = np.argmax(y_prob, axis=1)\nelse:\n    best_model = joblib.load(MODEL_DIR / f\"{best_model_name}.pkl\")\n    y_pred_best = best_model.predict(X_test)\n\n# Classification report\nprint(\"\\nDetailed Classification Report:\")\nprint(\"=\" * 80)\nreport = classification_report(y_test, y_pred_best, target_names=class_names, digits=4)\nprint(report)\n\n# Per-class F1 scores\nfrom sklearn.metrics import f1_score\nper_class_f1 = f1_score(y_test, y_pred_best, average=None)\n\n# Create DataFrame for analysis\nclass_performance = pd.DataFrame({\n    'class': class_names,\n    'f1_score': per_class_f1,\n    'support': [(y_test == i).sum() for i in range(num_classes)]\n})\nclass_performance = class_performance.sort_values('f1_score')\n\nprint(\"\\nPer-Class F1 Scores (sorted by performance):\")\nprint(class_performance.to_string(index=False))\n\n# Plot per-class F1 scores\nplt.figure(figsize=(12, 6))\ncolors = ['#e74c3c' if f1 < 0.95 else '#f39c12' if f1 < 0.98 else '#2ecc71' for f1 in class_performance['f1_score']]\nplt.barh(range(len(class_performance)), class_performance['f1_score'], color=colors)\nplt.yticks(range(len(class_performance)), class_performance['class'])\nplt.xlabel('F1 Score')\nplt.title(f'Per-Class F1 Scores - {best_model_name}')\nplt.axvline(x=0.95, color='red', linestyle='--', alpha=0.5, label='0.95 threshold')\nplt.axvline(x=0.98, color='orange', linestyle='--', alpha=0.5, label='0.98 threshold')\nplt.legend()\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.savefig(MODEL_DIR / 'per_class_performance.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nSaved per-class analysis to {MODEL_DIR / 'per_class_performance.png'}\")\n\n# Identify weak classes\nweak_classes = class_performance[class_performance['f1_score'] < 0.95]\nif len(weak_classes) > 0:\n    print(\"\\n⚠️  Classes with F1 < 0.95 (need improvement):\")\n    print(weak_classes.to_string(index=False))\nelse:\n    print(\"\\n✓ All classes have F1 >= 0.95!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "prqepcqeod",
   "source": "# ============================================================\n# Results Summary Table\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Model Comparison Summary\")\nprint(\"=\" * 60)\n\nres_df = pd.DataFrame(results).sort_values('f1_weighted', ascending=False).reset_index(drop=True)\n\n# Round for display\ndisplay_df = res_df.copy()\ndisplay_df['accuracy'] = display_df['accuracy'].apply(lambda x: f\"{x:.4f}\")\ndisplay_df['f1_weighted'] = display_df['f1_weighted'].apply(lambda x: f\"{x:.4f}\")\ndisplay_df['precision'] = display_df['precision'].apply(lambda x: f\"{x:.4f}\")\ndisplay_df['recall'] = display_df['recall'].apply(lambda x: f\"{x:.4f}\")\ndisplay_df['inference_time_sec'] = display_df['inference_time_sec'].apply(lambda x: f\"{x:.2f}\")\ndisplay_df['model_size_mb'] = display_df['model_size_mb'].apply(lambda x: f\"{x:.2f}\")\ndisplay_df['predictions_per_sec'] = display_df['predictions_per_sec'].apply(lambda x: f\"{x:.0f}\")\n\nprint(\"\\n\")\nprint(display_df.to_string(index=False))\n\n# Save comparison\nres_df.to_csv(MODEL_DIR / \"model_comparison_enhanced.csv\", index=False)\nprint(f\"\\nSaved detailed comparison to {MODEL_DIR / 'model_comparison_enhanced.csv'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}